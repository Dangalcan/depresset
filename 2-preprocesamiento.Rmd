---
title: "Estudio sobre la depresión estudiantil. Grupo 5. Preprocesamiento"
author: "Miguel Encina Martinez, Daniel Galván Cancio, Jaime Linares Barrera, Daniel Vela Camacho"
output:
  html_document:
    toc: true
    toc_float: true
    # number_sections: true
---

# Preprocesamiento

En este apartado vamos a realizar las transformaciones necesarias para dejar nuestro dataset listo para análisis más avanzados y modelado predictivo. Entre las tareas principales se incluyen:

-   Filtrado de participantes que no sean estudiantes.

-   Eliminación de columnas irrelevantes.

-   Recodificación de variables categóricas binarias (“Yes”/“No”) a numéricas.

-   Discretización y codificación de variables ordinales, como la duración del sueño.

-   Posible agrupación o recodificación de variables con niveles poco representativos.

-   Identificación y tratamiento de valores extremos en variables numéricas.

Antes de comenzar, vamos a importar las librerías que nos van a hacer falta en este notebook.

```{r}
#install.packages("dplyr")
library(dplyr)

#install.packages("stringr")
library(stringr)

#install.packages("caret")
library(caret)
```

## 1. Lectura del dataset

Leemos el archivo CSV `student_depression_dataset` que se encuentra en la carpeta `data`.

```{r}
data <- read.csv("./data/student_depression_dataset.csv", sep=",")
head(data)
```

## 2. Filtrado por estudiantes y eliminación de columnas innecesarias

Como hemos comentado en la parte de visualización del dataset, la mayoría de participantes son estudiantes, por lo que vamos a hacer un filtro para quedarnos únicamente con los estudiantes y vamos a eliminar aquellas columnas que no van a aportarnos ninguna información como son:

-   `id`: identificador sin utilidad analítica.
-   `Work.Pressure`: constante en estudiantes (valor 0 en todos los casos).
-   `Job.Satisfaction`: constante en estudiantes (valor 0 en todos los casos).
-   `Profession`: vamos a trabajar solo con estudiantes

```{r}
df_students <- data %>%
  filter(Profession == "Student") %>%
  select(-c(Profession, id, Work.Pressure, Job.Satisfaction))

head(df_students)
```

## 3. Renombrar variables

Vamos a renombrar el nombre de cierta variables para que sea más comodo trabajar con ellas.

```{r}
df_students <- df_students %>%
  rename(Had.suicidal.thoughts=Have.you.ever.had.suicidal.thoughts.., 
         Family.History.Mental.Illness=Family.History.of.Mental.Illness)
```

## 4. Limpieza de valores nulos no estándar

Hemos detectado que la variable `Financial.Stress` contiene valores representados por el carácter `?`. Estos valores constituyen ruido y no permiten la conversión numérica. Procedemos a filtrar estos registros antes de transformar la variable, siguiendo las prácticas de limpieza de datos para eliminar valores inconsistentes.

```{r}

df_students <- df_students %>%
  filter(Financial.Stress != "?") %>%
  mutate(Financial.Stress = as.numeric(Financial.Stress))

```

## 5. Codificación de variables booleanas

Transformamos las variables categóricas binarias ("Yes"/"No") a una representación numérica (1/0) para que puedan ser procesadas matemáticamente por los algoritmos de modelado posteriores.

```{r}

df_students <- df_students %>%
  mutate(
    Had.suicidal.thoughts = ifelse(Had.suicidal.thoughts == "Yes", 1, 0),
    Family.History.Mental.Illness = ifelse(Family.History.Mental.Illness == "Yes", 1, 0),
    Depression = ifelse(Depression == "Yes", 1, 0)
  )

```

### 6. Tratamiento de variables ordinales y limpieza de texto

La variable **Sleep.Duration** presenta inconsistencias de formato: los valores están encerrados entre comillas (ej. 'Less than 5 hours') y existe una categoría "Others" no informativa. Dado que la manipulación de cadenas en R base puede resultar compleja, utilizaremos la **librería stringr** (mencionada como recurso recomendado en la documentación de R Base) para limpiar estos caracteres antes de la conversión.

El proceso consiste en:

1.  Eliminar cualquier tipo de comillas (simples o dobles) de la cadena.

2.  Filtrar y eliminar los registros que, tras la limpieza, resulten ser "Others".

3.  Convertir la variable limpia a factor ordenado.

```{r}

df_students <- df_students %>%
  mutate(Sleep.Duration = str_remove_all(Sleep.Duration, "['\"]")) %>%
  filter(Sleep.Duration != "Others") %>%
  mutate(
    Sleep.Duration = factor(Sleep.Duration, 
                            levels = c("Less than 5 hours", "5-6 hours", "7-8 hours", "More than 8 hours"), 
                            ordered = TRUE)
  )


```

Aplicamos una lógica similar para **Dietary.Habits**, estableciendo un orden de calidad en la dieta y eliminando registros no informativos si los hubiera.

```{r}

df_students <- df_students %>%
  filter(Dietary.Habits != "Others") %>%
  mutate(
    Dietary.Habits = factor(Dietary.Habits, 
                            levels = c("Unhealthy", "Moderate", "Healthy"), 
                            ordered = TRUE)
  )

```

### 7. Reducción de cardinalidad en variables categóricas

La variable Degree posee un **alto número de categorías con baja frecuencia**, lo que puede dificultar el aprendizaje de los modelos. Agrupamos las titulaciones minoritarias bajo la etiqueta "Other", manteniendo únicamente las 10 más frecuentes. Convertimos también Gender y City a **factores** para su correcta gestión en el primer dataset.

```{r}

top_degrees <- df_students %>%
  count(Degree) %>%
  top_n(10, wt = n) %>%
  pull(Degree)

df_students <- df_students %>%
  mutate(
    Degree = ifelse(Degree %in% top_degrees, Degree, "Other"),
    Degree = as.factor(Degree),
    Gender = as.factor(Gender),
    City = as.factor(City)
  )

```

### 8. Gestión de outliers

Para asegurar la calidad de los datos numéricos, identificamos y eliminamos valores atípicos en **Age** y **CGPA** utilizando el criterio del **Rango Intercuartílico** (IQR). Esto evita que valores extremos sesguen los resultados de modelos sensibles a la distancia.

```{r}

numeric_cols_outliers <- c("Age", "CGPA")

remove_outliers <- function(data, variable) {
  Q1 <- quantile(data[[variable]], 0.25, na.rm = TRUE)
  Q3 <- quantile(data[[variable]], 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  data %>% filter(data[[variable]] >= lower_bound & data[[variable]] <= upper_bound)
}

for (col in numeric_cols_outliers) {
  df_students <- remove_outliers(df_students, col)
}

```

### 9. Generación de Datasets para Minería de Datos

En esta fase final del preprocesamiento, generaremos las distintas versiones del dataset necesarias para las tareas de minería de datos. Además, aplicaremos una Selección de atributos basada en filtros para eliminar variables redundantes generadas por la codificación *dummy*, mejorando así la calidad de los datos antes de guardarlos

## 9.1 Dataset 1: Variables como factores

Esta versión mantiene las variables categóricas y ordinales como factores de R. Es la versión adecuada para algoritmos basados en árboles de decisión (rpart) y para visualización semántica, donde la multicolinealidad no es tan crítica o el propio algoritmo la gestiona.

```{r}
df_factors <- df_students
write.csv(df_factors, "./data/student_depression_clean_factors.csv", row.names = FALSE)
```

## 9.2 Dataset 2: Codificación numérica y selección de atributos

A continuación, procedemos a transformar los datos para obtener una matriz puramente numérica. En primer lugar, convertimos las variables ordinales (Sleep.Duration y Dietary.Habits) a su representación entera, preservando el orden implícito que poseen.

```{r}

df_numeric_temp <- df_factors %>%
  mutate(
    Sleep.Duration = as.numeric(Sleep.Duration),
    Dietary.Habits = as.numeric(Dietary.Habits)
  )

```

Las **variables nominales** (Gender, City, Degree) no tienen un orden matemático, por lo que utilizamos la técnica de **One-Hot Encoding** (variables dummy). Esto expande cada categoría en una nueva columna binaria. Utilizamos la función dummyVars del paquete caret para crear este modelo de transformación.

```{r}

dummies_model <- dummyVars(~ Gender + City + Degree, data = df_numeric_temp)
df_dummies <- predict(dummies_model, newdata = df_numeric_temp)
df_dummies <- as.data.frame(df_dummies)

```

Unimos las nuevas variables dummy al dataset original numérico y eliminamos las columnas categóricas originales, ya que han sido sustituidas.

```{r}

df_numeric_full <- cbind(df_numeric_temp, df_dummies) %>%
  select(-Gender, -City, -Degree)

```

Al generar variables dummy, es común introducir redundancia (multicolinealidad) en los datos. Para mitigar esto, aplicamos una técnica de Selección de Atributos (Filtro). Primero, calculamos la matriz de correlación de todas las variables predictoras, excluyendo la variable objetivo (Depression).

```{r}

cor_matrix <- cor(df_numeric_full[, setdiff(names(df_numeric_full), "Depression")])

```

Utilizamos la función findCorrelation de caret para identificar automáticamente las variables que tienen una correlación absoluta superior a 0.75. Esta función nos devuelve los índices de las columnas que deben eliminarse para reducir la redundancia.

```{r}

high_cor_indices <- findCorrelation(cor_matrix, cutoff = 0.75)
redundant_vars <- colnames(cor_matrix)[high_cor_indices]

cat("Variables eliminadas por alta correlación (> 0.75):\n")
print(redundant_vars)

```

Tras aplicar el filtro de correlación con un umbral de 0.75, el algoritmo ha identificado y eliminado la variable `Gender.Female`. Esto es un comportamiento esperado y deseable. Al realizar la codificación *One-Hot* sobre la variable binaria original `Gender` (Male/Female), se generaron dos columnas (`Gender.Male` y `Gender.Female`) que son perfectamente colineales (si un estudiante no es hombre, necesariamente es mujer, y viceversa).

Mantener ambas columnas introduciría **multicolinealidad perfecta**, lo que puede desestabilizar modelos lineales como la Regresión Logística. La eliminación automática de una de ellas valida la eficacia de este paso de preprocesamiento.

Finalmente, eliminamos dichas variables redundantes del dataset numérico y guardamos el resultado optimizado en un archivo CSV.

```{r}

df_numeric_final <- df_numeric_full %>% select(-all_of(redundant_vars))
write.csv(df_numeric_final, "./data/student_depression_numeric.csv", row.names = FALSE)

```

## 9.3 Dataset 3: Normalización (Min-Max)

Observando el resumen estadístico del dataset numérico, vemos variables con rangos muy dispares: `Age` varía aproximadamente entre 18 y 60, mientras que `CGPA` lo hace entre 0 y 10, y las variables binarias entre 0 y 1.

Para algoritmos basados en el cálculo de distancias (como k-NN, que utilizaremos en la siguiente fase) o basados en el descenso de gradiente (como Redes Neuronales), esta disparidad haría que la variable `Age` dominara el cálculo de la distancia sobre el resto. Al aplicar la normalización **Min-Max**, transformamos todas las variables predictoras al rango $[0, 1]$, asegurando que todas contribuyan equitativamente al modelo. La variable objetivo `Depression` se excluye de este proceso para mantener su naturaleza binaria original.

```{r}

cols_to_scale <- setdiff(names(df_numeric_final), "Depression")

```

Calculamos los parámetros de escalado utilizando la función preProcess de caret con el método "range", que aplica una normalización Min-Max para llevar los valores al intervalo [0, 1].

```{r}

process_minmax <- preProcess(df_numeric_final[, cols_to_scale], method = c("range"))

```

Aplicamos la transformación calculada a los datos.

```{r}

df_normalized_values <- predict(process_minmax, df_numeric_final[, cols_to_scale])

```

Reconstruimos el dataset final añadiendo de nuevo la variable objetivo Depression (que no debe ser escalada ya que es binaria) y guardamos el archivo CSV definitivo.

```{r}

df_normalized_final <- df_normalized_values %>%
  mutate(Depression = df_numeric_final$Depression)

write.csv(df_normalized_final, "./data/student_depression_normalized.csv", row.names = FALSE)

```

El preprocesamiento ha concluido con éxito. Mostramos un resumen de las dimensiones y estructura del dataset final normalizado.

```{r}

cat("Dimensiones finales (filas, columnas):", dim(df_normalized_final))
summary(df_normalized_final)

```

### 10. Conclusiones

Tras ejecutar el pipeline completo de limpieza y transformación, hemos obtenido un conjunto de datos robusto y preparado para la fase de modelado. A partir del análisis del dataset final (df_normalized), extraemos las siguientes conclusiones técnicas:

1.  Reducción de Dimensionalidad Controlada:

-   Partíamos de un dataset crudo con variables ruidosas. Tras la limpieza, el One-Hot Encoding y la selección de atributos, hemos consolidado un dataset con 75 columnas.

-   Se ha mitigado la explosión de dimensionalidad derivada de la variable Degree agrupando las titulaciones minoritarias, lo que evita la dispersión de datos (sparsity).

-   Se ha eliminado la redundancia introducida por las variables dummy binarias (ej. Género).

2.  Calidad del Dato:

-   No existen valores perdidos (NAs) ni caracteres extraños.

-   Los valores atípicos (outliers) en Age y CGPA han sido filtrados utilizando el criterio IQR, lo que resultará en modelos más estables y menos sesgados por casos extremos.

3.  Estado de la Variable Objetivo (Depression):

-   Observando la media de la variable Depression en el resumen estadístico (Mean: 0.5855), concluimos que el dataset está razonablemente balanceado, con un 58.5% de casos positivos (estudiantes con depresión) frente a un 41.5% de casos negativos. Esto es una excelente noticia, ya que implica que no será estrictamente necesario aplicar técnicas de balanceo de clases (como SMOTE o Undersampling) en la fase de modelado, simplificando el flujo de trabajo.

4.  Disponibilidad de Activos: Hemos exportado exitosamente tres versiones de los datos en la carpeta /data, cumpliendo con los requisitos de las distintas familias de algoritmos que exploraremos en los siguientes cuadernos:

-   Árboles y Reglas: Utilizarán student_depression_clean_factors.csv.

-   Modelos Probabilísticos: Podrán usar student_depression_numeric.csv.

-   Modelos de Distancia (k-NN, SVM, Redes): Utilizarán student_depression_normalized.csv.

```{r}

```

```{r}

```
